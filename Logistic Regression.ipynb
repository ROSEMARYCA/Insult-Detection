{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to sbhat1@scu.edu and will expire on August 12, 2018.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1503046013.log\n"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "test_data = gl.SFrame.read_csv(\"/Users/swastika.b/Documents/Swastika/SantaClaraEdu/courses/Machine Learning/Project/FinalCode/train.csv\",verbose= False)\n",
    "data = gl.SFrame.read_csv(\"/Users/swastika.b/Documents/Swastika/SantaClaraEdu/courses/Machine Learning/Project/FinalCode/training_data.csv\",verbose= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing DataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    \n",
    "    import re\n",
    "    import string\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    \n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    lower_case_text = text.translate(None, string.punctuation).lower()\n",
    "    words_list = word_tokenize(lower_case_text)\n",
    "    text2 = []\n",
    "    for word in words_list:\n",
    "        text0 = word.decode('ascii', 'ignore')\n",
    "        text1 = regex.sub(u'', text0)\n",
    "        if not text1 == u'':\n",
    "            if not text1 in stopwords.words('english'):\n",
    "                text2.append(porter.stem(text1))\n",
    "    return text2\n",
    "    \n",
    "\n",
    "data['comment_clean'] = data['Comment'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_text(words):\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['comment_string'] = data['comment_clean'].apply(create_text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    lower_case_text = text.translate(None, string.punctuation).lower()\n",
    "    return lower_case_text\n",
    "    \n",
    "#data['comment_string'] = data['Comment'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def split_into_lemmas(comments):\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 4), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    analyze = bigram_vectorizer.build_analyzer()\n",
    "    return analyze(comments)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=split_into_lemmas,stop_words='english',strip_accents='ascii').fit(data['comment_string'])\n",
    "text_transformed = vectorizer.transform(data['comment_string'])\n",
    "tfidf_transformer = TfidfTransformer().fit(text_transformed)\n",
    "tfidf_transformed_text = tfidf_transformer.transform(text_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression(C=2)\n",
    "lr.fit(tfidf_transformed_text,data['Insult'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K- Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70693512304250561"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "y = (data['Insult']).to_numpy()\n",
    "predicted = cross_val_predict(lr, tfidf_transformed_text, y, cv=10)\n",
    "metrics.accuracy_score(y, predicted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['comment_clean'] = test_data['Comment'].apply(transform_text)\n",
    "test_data['comment_string'] = test_data['comment_clean'].apply(create_text)\n",
    "#test_data['comment_string'] = test_data['Comment'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_text_transformed = vectorizer.transform(test_data['comment_string'])\n",
    "tfidf_transformed_test_text = tfidf_transformer.transform(test_text_transformed)\n",
    "predicted_class = lr.predict(tfidf_transformed_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70230554851786164"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "true_class = np.array(test_data['Insult'])\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_class, predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2235"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
